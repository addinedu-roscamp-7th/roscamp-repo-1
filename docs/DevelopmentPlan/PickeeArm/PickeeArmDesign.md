# Pickee Arm 상세 설계 (1단계: 수동 지정 좌표 기반)

## 1. 개요

이 문서는 `pickee_arm` ROS2 패키지의 **1단계: 수동 지정 좌표 기반 제어** 방식에 대한 상세 설계를 다룬다. 이 설계의 목표는 빠르고 안정적인 초기 기능 구현에 있다.

`pickee_arm`은 `pickee_main`으로부터 작업을 지시받아, 미리 정의된 관절 각도 시퀀스를 실행하는 단순 실행기(Actuator) 역할을 한다.

## 2. 1단계 상세 설계: 수동 지정 좌표 컨트롤러

### 2.1. 아키텍처
`pickee_arm`은 `pickee_main`으로부터 서비스 요청을 받아 동작하고, 토픽을 통해 상태를 보고한다. 로봇 팔 하드웨어와는 직접 통신하며, 이 설계에서는 카메라를 사용하지 않는다.

### 2.2. 핵심 설계: 수동 지정 포즈 시퀀스
- **포즈 정의**: 로봇 팔의 주요 동작에 필요한 6축 관절 각도 값들을, 사용자가 실제 로봇을 움직여 찾은 후 파이썬 코드 내에 상수로 정의한다.
- **시퀀스 실행**: 서비스가 호출되면, 컨트롤러는 정의된 포즈들을 정해진 순서대로 로봇 드라이버에 전송하여 동작 시퀀스를 수행한다.

### 2.3. 구현 상세 (`pickee_arm_controller.py`)
- **`PickeeArmController` 클래스**: 모든 ROS2 통신과 동작 시퀀스 실행 로직을 포함하는 단일 노드 클래스이다.
- **상수**: 파일 상단에 `STANDBY_POSE`, `PICK_SEQUENCE` 등 사용자가 직접 측정한 관절 각도 값들을 정의한다.
- **`_move_to_target_joints()` 메서드**: 단일 목표 관절 각도로 팔을 움직이는 내부 헬퍼 함수이다.

## 3. 2단계 상세 설계: Deep Learning-Based Controller

### 3.1. 아키텍처 개요
최종 단계에서는 이미지로부터 직접 6-DOF 관절 각도를 회귀 예측하는 CNN 모델을 기반으로 로봇 팔을 제어한다. 이 과정은 **오프라인 학습**과 **온라인 추론**으로 분리된다.

### 3.2. 오프라인 학습 과정
- **`DataCollector` 스크립트**: ROS2 환경 외부에서, 로봇 팔을 움직이며 (이미지, 관절각도) 데이터 쌍을 수집하여 CSV 파일로 저장한다.
- **`train.py` 스크립트**: 수집된 데이터셋을 사용하여 PyTorch와 ResNet-18 기반의 `PoseCNN` 모델을 학습시키고, 가중치 파일(`best.pt`)을 생성한다.

### 3.3. 온라인 추론 및 제어 설계
- **데이터 흐름**:
  ```mermaid
  graph TD
      A["카메라 이미지 입력"] --> B{"YOLO 객체 탐지"};
      B --> C["잘라낸 이미지 전달"];
      C --> D{"PoseCNN 모델"};
      D --> E["6-DOF 관절 각도 예측"];
      E --> F{"제어 로직"};
      F --> G["로봇 팔 드라이버"];
  ```
- **주요 패키지 내 컴포넌트**:
  - **`PickeeArmController` (추론 노드)**: `best.pt` 모델 파일을 로드한다. 실시간 카메라 이미지를 받아 전처리한 후, 모델에 입력하여 관절 각도를 추론하고 로봇 드라이버에 명령을 내린다.
  - **`PoseCNN` (모델 정의 클래스)**: `best.pt` 가중치를 로드하기 위해, 학습 때 사용했던 것과 동일한 신경망 구조를 정의하는 클래스.
  - **모델 가중치 파일**: 오프라인 학습 과정에서 생성된 `best.pt` 파일. 패키지 내에 함께 배포되어야 한다.